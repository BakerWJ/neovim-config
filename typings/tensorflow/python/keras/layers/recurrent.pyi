from tensorflow.python.eager import context as context
from tensorflow.python.framework import ops as ops, tensor_shape as tensor_shape
from tensorflow.python.keras import activations as activations, constraints as constraints, initializers as initializers, regularizers as regularizers
from tensorflow.python.keras.engine.base_layer import Layer as Layer
from tensorflow.python.keras.engine.input_spec import InputSpec as InputSpec
from tensorflow.python.keras.saving.saved_model import layer_serialization as layer_serialization
from tensorflow.python.keras.utils import generic_utils as generic_utils, tf_utils as tf_utils
from tensorflow.python.ops import array_ops as array_ops, control_flow_ops as control_flow_ops, control_flow_util as control_flow_util, math_ops as math_ops, state_ops as state_ops
from tensorflow.python.training.tracking import data_structures as data_structures
from tensorflow.python.util import nest as nest
from tensorflow.python.util.tf_export import keras_export as keras_export
from tensorflow.tools.docs import doc_controls as doc_controls
from typing import Any, Optional

RECURRENT_DROPOUT_WARNING_MSG: str

class StackedRNNCells(Layer):
    cells: Any = ...
    reverse_state_order: Any = ...
    def __init__(self, cells: Any, **kwargs: Any) -> None: ...
    @property
    def state_size(self): ...
    @property
    def output_size(self): ...
    def get_initial_state(self, inputs: Optional[Any] = ..., batch_size: Optional[Any] = ..., dtype: Optional[Any] = ...): ...
    def call(self, inputs: Any, states: Any, constants: Optional[Any] = ..., training: Optional[Any] = ..., **kwargs: Any): ...
    built: bool = ...
    def build(self, input_shape: Any) -> None: ...
    def get_config(self): ...
    @classmethod
    def from_config(cls, config: Any, custom_objects: Optional[Any] = ...): ...

class RNN(Layer):
    zero_output_for_mask: Any = ...
    cell: Any = ...
    return_sequences: Any = ...
    return_state: Any = ...
    go_backwards: Any = ...
    stateful: Any = ...
    unroll: Any = ...
    time_major: Any = ...
    supports_masking: bool = ...
    input_spec: Any = ...
    state_spec: Any = ...
    constants_spec: Any = ...
    def __init__(self, cell: Any, return_sequences: bool = ..., return_state: bool = ..., go_backwards: bool = ..., stateful: bool = ..., unroll: bool = ..., time_major: bool = ..., **kwargs: Any) -> None: ...
    @property
    def states(self): ...
    @states.setter
    def states(self, states: Any) -> None: ...
    def compute_output_shape(self, input_shape: Any): ...
    def compute_mask(self, inputs: Any, mask: Any): ...
    built: bool = ...
    def build(self, input_shape: Any): ...
    def get_initial_state(self, inputs: Any): ...
    def __call__(self, inputs: Any, initial_state: Optional[Any] = ..., constants: Optional[Any] = ..., **kwargs: Any): ...
    def call(self, inputs: Any, mask: Optional[Any] = ..., training: Optional[Any] = ..., initial_state: Optional[Any] = ..., constants: Optional[Any] = ...): ...
    def reset_states(self, states: Optional[Any] = ...): ...
    def get_config(self): ...
    @classmethod
    def from_config(cls, config: Any, custom_objects: Optional[Any] = ...): ...

class AbstractRNNCell(Layer):
    def call(self, inputs: Any, states: Any) -> None: ...
    @property
    def state_size(self) -> None: ...
    @property
    def output_size(self) -> None: ...
    def get_initial_state(self, inputs: Optional[Any] = ..., batch_size: Optional[Any] = ..., dtype: Optional[Any] = ...): ...

class DropoutRNNCellMixin:
    def __init__(self, *args: Any, **kwargs: Any) -> None: ...
    def reset_dropout_mask(self) -> None: ...
    def reset_recurrent_dropout_mask(self) -> None: ...
    def get_dropout_mask_for_cell(self, inputs: Any, training: Any, count: int = ...): ...
    def get_recurrent_dropout_mask_for_cell(self, inputs: Any, training: Any, count: int = ...): ...

class SimpleRNNCell(DropoutRNNCellMixin, Layer):
    units: Any = ...
    activation: Any = ...
    use_bias: Any = ...
    kernel_initializer: Any = ...
    recurrent_initializer: Any = ...
    bias_initializer: Any = ...
    kernel_regularizer: Any = ...
    recurrent_regularizer: Any = ...
    bias_regularizer: Any = ...
    kernel_constraint: Any = ...
    recurrent_constraint: Any = ...
    bias_constraint: Any = ...
    dropout: Any = ...
    recurrent_dropout: Any = ...
    state_size: Any = ...
    output_size: Any = ...
    def __init__(self, units: Any, activation: str = ..., use_bias: bool = ..., kernel_initializer: str = ..., recurrent_initializer: str = ..., bias_initializer: str = ..., kernel_regularizer: Optional[Any] = ..., recurrent_regularizer: Optional[Any] = ..., bias_regularizer: Optional[Any] = ..., kernel_constraint: Optional[Any] = ..., recurrent_constraint: Optional[Any] = ..., bias_constraint: Optional[Any] = ..., dropout: float = ..., recurrent_dropout: float = ..., **kwargs: Any) -> None: ...
    kernel: Any = ...
    recurrent_kernel: Any = ...
    bias: Any = ...
    built: bool = ...
    def build(self, input_shape: Any) -> None: ...
    def call(self, inputs: Any, states: Any, training: Optional[Any] = ...): ...
    def get_initial_state(self, inputs: Optional[Any] = ..., batch_size: Optional[Any] = ..., dtype: Optional[Any] = ...): ...
    def get_config(self): ...

class SimpleRNN(RNN):
    activity_regularizer: Any = ...
    input_spec: Any = ...
    def __init__(self, units: Any, activation: str = ..., use_bias: bool = ..., kernel_initializer: str = ..., recurrent_initializer: str = ..., bias_initializer: str = ..., kernel_regularizer: Optional[Any] = ..., recurrent_regularizer: Optional[Any] = ..., bias_regularizer: Optional[Any] = ..., activity_regularizer: Optional[Any] = ..., kernel_constraint: Optional[Any] = ..., recurrent_constraint: Optional[Any] = ..., bias_constraint: Optional[Any] = ..., dropout: float = ..., recurrent_dropout: float = ..., return_sequences: bool = ..., return_state: bool = ..., go_backwards: bool = ..., stateful: bool = ..., unroll: bool = ..., **kwargs: Any) -> None: ...
    def call(self, inputs: Any, mask: Optional[Any] = ..., training: Optional[Any] = ..., initial_state: Optional[Any] = ...): ...
    @property
    def units(self): ...
    @property
    def activation(self): ...
    @property
    def use_bias(self): ...
    @property
    def kernel_initializer(self): ...
    @property
    def recurrent_initializer(self): ...
    @property
    def bias_initializer(self): ...
    @property
    def kernel_regularizer(self): ...
    @property
    def recurrent_regularizer(self): ...
    @property
    def bias_regularizer(self): ...
    @property
    def kernel_constraint(self): ...
    @property
    def recurrent_constraint(self): ...
    @property
    def bias_constraint(self): ...
    @property
    def dropout(self): ...
    @property
    def recurrent_dropout(self): ...
    def get_config(self): ...
    @classmethod
    def from_config(cls, config: Any): ...

class GRUCell(DropoutRNNCellMixin, Layer):
    units: Any = ...
    activation: Any = ...
    recurrent_activation: Any = ...
    use_bias: Any = ...
    kernel_initializer: Any = ...
    recurrent_initializer: Any = ...
    bias_initializer: Any = ...
    kernel_regularizer: Any = ...
    recurrent_regularizer: Any = ...
    bias_regularizer: Any = ...
    kernel_constraint: Any = ...
    recurrent_constraint: Any = ...
    bias_constraint: Any = ...
    dropout: Any = ...
    recurrent_dropout: Any = ...
    implementation: int = ...
    reset_after: Any = ...
    state_size: Any = ...
    output_size: Any = ...
    def __init__(self, units: Any, activation: str = ..., recurrent_activation: str = ..., use_bias: bool = ..., kernel_initializer: str = ..., recurrent_initializer: str = ..., bias_initializer: str = ..., kernel_regularizer: Optional[Any] = ..., recurrent_regularizer: Optional[Any] = ..., bias_regularizer: Optional[Any] = ..., kernel_constraint: Optional[Any] = ..., recurrent_constraint: Optional[Any] = ..., bias_constraint: Optional[Any] = ..., dropout: float = ..., recurrent_dropout: float = ..., implementation: int = ..., reset_after: bool = ..., **kwargs: Any) -> None: ...
    kernel: Any = ...
    recurrent_kernel: Any = ...
    bias: Any = ...
    built: bool = ...
    def build(self, input_shape: Any) -> None: ...
    def call(self, inputs: Any, states: Any, training: Optional[Any] = ...): ...
    def get_config(self): ...
    def get_initial_state(self, inputs: Optional[Any] = ..., batch_size: Optional[Any] = ..., dtype: Optional[Any] = ...): ...

class GRU(RNN):
    activity_regularizer: Any = ...
    input_spec: Any = ...
    def __init__(self, units: Any, activation: str = ..., recurrent_activation: str = ..., use_bias: bool = ..., kernel_initializer: str = ..., recurrent_initializer: str = ..., bias_initializer: str = ..., kernel_regularizer: Optional[Any] = ..., recurrent_regularizer: Optional[Any] = ..., bias_regularizer: Optional[Any] = ..., activity_regularizer: Optional[Any] = ..., kernel_constraint: Optional[Any] = ..., recurrent_constraint: Optional[Any] = ..., bias_constraint: Optional[Any] = ..., dropout: float = ..., recurrent_dropout: float = ..., implementation: int = ..., return_sequences: bool = ..., return_state: bool = ..., go_backwards: bool = ..., stateful: bool = ..., unroll: bool = ..., reset_after: bool = ..., **kwargs: Any) -> None: ...
    def call(self, inputs: Any, mask: Optional[Any] = ..., training: Optional[Any] = ..., initial_state: Optional[Any] = ...): ...
    @property
    def units(self): ...
    @property
    def activation(self): ...
    @property
    def recurrent_activation(self): ...
    @property
    def use_bias(self): ...
    @property
    def kernel_initializer(self): ...
    @property
    def recurrent_initializer(self): ...
    @property
    def bias_initializer(self): ...
    @property
    def kernel_regularizer(self): ...
    @property
    def recurrent_regularizer(self): ...
    @property
    def bias_regularizer(self): ...
    @property
    def kernel_constraint(self): ...
    @property
    def recurrent_constraint(self): ...
    @property
    def bias_constraint(self): ...
    @property
    def dropout(self): ...
    @property
    def recurrent_dropout(self): ...
    @property
    def implementation(self): ...
    @property
    def reset_after(self): ...
    def get_config(self): ...
    @classmethod
    def from_config(cls, config: Any): ...

class LSTMCell(DropoutRNNCellMixin, Layer):
    units: Any = ...
    activation: Any = ...
    recurrent_activation: Any = ...
    use_bias: Any = ...
    kernel_initializer: Any = ...
    recurrent_initializer: Any = ...
    bias_initializer: Any = ...
    unit_forget_bias: Any = ...
    kernel_regularizer: Any = ...
    recurrent_regularizer: Any = ...
    bias_regularizer: Any = ...
    kernel_constraint: Any = ...
    recurrent_constraint: Any = ...
    bias_constraint: Any = ...
    dropout: Any = ...
    recurrent_dropout: Any = ...
    implementation: int = ...
    state_size: Any = ...
    output_size: Any = ...
    def __init__(self, units: Any, activation: str = ..., recurrent_activation: str = ..., use_bias: bool = ..., kernel_initializer: str = ..., recurrent_initializer: str = ..., bias_initializer: str = ..., unit_forget_bias: bool = ..., kernel_regularizer: Optional[Any] = ..., recurrent_regularizer: Optional[Any] = ..., bias_regularizer: Optional[Any] = ..., kernel_constraint: Optional[Any] = ..., recurrent_constraint: Optional[Any] = ..., bias_constraint: Optional[Any] = ..., dropout: float = ..., recurrent_dropout: float = ..., implementation: int = ..., **kwargs: Any) -> None: ...
    kernel: Any = ...
    recurrent_kernel: Any = ...
    bias: Any = ...
    built: bool = ...
    def build(self, input_shape: Any): ...
    def call(self, inputs: Any, states: Any, training: Optional[Any] = ...): ...
    def get_config(self): ...
    def get_initial_state(self, inputs: Optional[Any] = ..., batch_size: Optional[Any] = ..., dtype: Optional[Any] = ...): ...

class PeepholeLSTMCell(LSTMCell):
    input_gate_peephole_weights: Any = ...
    forget_gate_peephole_weights: Any = ...
    output_gate_peephole_weights: Any = ...
    def build(self, input_shape: Any) -> None: ...

class LSTM(RNN):
    activity_regularizer: Any = ...
    input_spec: Any = ...
    def __init__(self, units: Any, activation: str = ..., recurrent_activation: str = ..., use_bias: bool = ..., kernel_initializer: str = ..., recurrent_initializer: str = ..., bias_initializer: str = ..., unit_forget_bias: bool = ..., kernel_regularizer: Optional[Any] = ..., recurrent_regularizer: Optional[Any] = ..., bias_regularizer: Optional[Any] = ..., activity_regularizer: Optional[Any] = ..., kernel_constraint: Optional[Any] = ..., recurrent_constraint: Optional[Any] = ..., bias_constraint: Optional[Any] = ..., dropout: float = ..., recurrent_dropout: float = ..., implementation: int = ..., return_sequences: bool = ..., return_state: bool = ..., go_backwards: bool = ..., stateful: bool = ..., unroll: bool = ..., **kwargs: Any) -> None: ...
    def call(self, inputs: Any, mask: Optional[Any] = ..., training: Optional[Any] = ..., initial_state: Optional[Any] = ...): ...
    @property
    def units(self): ...
    @property
    def activation(self): ...
    @property
    def recurrent_activation(self): ...
    @property
    def use_bias(self): ...
    @property
    def kernel_initializer(self): ...
    @property
    def recurrent_initializer(self): ...
    @property
    def bias_initializer(self): ...
    @property
    def unit_forget_bias(self): ...
    @property
    def kernel_regularizer(self): ...
    @property
    def recurrent_regularizer(self): ...
    @property
    def bias_regularizer(self): ...
    @property
    def kernel_constraint(self): ...
    @property
    def recurrent_constraint(self): ...
    @property
    def bias_constraint(self): ...
    @property
    def dropout(self): ...
    @property
    def recurrent_dropout(self): ...
    @property
    def implementation(self): ...
    def get_config(self): ...
    @classmethod
    def from_config(cls, config: Any): ...
