from tensorflow.python.data.ops import dataset_ops as dataset_ops
from tensorflow.python.framework import dtypes as dtypes, ops as ops, tensor_shape as tensor_shape, tensor_spec as tensor_spec
from tensorflow.python.keras.engine.base_preprocessing_layer import Combiner as Combiner, CombinerPreprocessingLayer as CombinerPreprocessingLayer
from tensorflow.python.keras.layers.preprocessing import categorical_encoding as categorical_encoding, index_lookup as index_lookup
from tensorflow.python.keras.utils import layer_utils as layer_utils
from tensorflow.python.ops import array_ops as array_ops, control_flow_ops as control_flow_ops, gen_string_ops as gen_string_ops, string_ops as string_ops
from tensorflow.python.ops.ragged import ragged_functional_ops as ragged_functional_ops, ragged_string_ops as ragged_string_ops, ragged_tensor as ragged_tensor
from tensorflow.python.util import compat as compat
from tensorflow.python.util.tf_export import keras_export as keras_export
from typing import Any, Optional

LOWER_AND_STRIP_PUNCTUATION: str
SPLIT_ON_WHITESPACE: str
TFIDF: Any
INT: Any
BINARY: Any
COUNT: Any
DEFAULT_STRIP_REGEX: str

class TextVectorization(CombinerPreprocessingLayer):
    def __init__(self, max_tokens: Optional[Any] = ..., standardize: Any = ..., split: Any = ..., ngrams: Optional[Any] = ..., output_mode: Any = ..., output_sequence_length: Optional[Any] = ..., pad_to_max_tokens: bool = ..., **kwargs: Any) -> None: ...
    def compute_output_shape(self, input_shape: Any): ...
    def compute_output_signature(self, input_spec: Any): ...
    def adapt(self, data: Any, reset_state: bool = ...): ...
    def get_vocabulary(self): ...
    def get_config(self): ...
    def count_params(self): ...
    def set_vocabulary(self, vocab: Any, df_data: Optional[Any] = ..., oov_df_value: Optional[Any] = ..., append: bool = ...) -> None: ...
    def build(self, input_shape: Any) -> None: ...
    def call(self, inputs: Any): ...

class _TextVectorizationAccumulator: ...

class _TextVectorizationCombiner(Combiner):
    def __init__(self, vocab_size: Optional[Any] = ..., compute_idf: bool = ...) -> None: ...
    def compute(self, values: Any, accumulator: Optional[Any] = ...): ...
    def merge(self, accumulators: Any): ...
    def extract(self, accumulator: Any): ...
    def restore(self, output: Any) -> None: ...
    def serialize(self, accumulator: Any): ...
    def deserialize(self, encoded_accumulator: Any): ...
