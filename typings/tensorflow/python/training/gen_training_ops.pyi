from tensorflow.python.util.deprecation import deprecated_endpoints as deprecated_endpoints
from tensorflow.python.util.tf_export import tf_export as tf_export
from typing import Any, Optional

def apply_ada_max(var: Any, m: Any, v: Any, beta1_power: Any, lr: Any, beta1: Any, beta2: Any, epsilon: Any, grad: Any, use_locking: bool = ..., name: Optional[Any] = ...): ...

ApplyAdaMax: Any

def apply_ada_max_eager_fallback(var: Any, m: Any, v: Any, beta1_power: Any, lr: Any, beta1: Any, beta2: Any, epsilon: Any, grad: Any, use_locking: Any, name: Any, ctx: Any) -> None: ...
def apply_adadelta(var: Any, accum: Any, accum_update: Any, lr: Any, rho: Any, epsilon: Any, grad: Any, use_locking: bool = ..., name: Optional[Any] = ...): ...

ApplyAdadelta: Any

def apply_adadelta_eager_fallback(var: Any, accum: Any, accum_update: Any, lr: Any, rho: Any, epsilon: Any, grad: Any, use_locking: Any, name: Any, ctx: Any) -> None: ...
def apply_adagrad(var: Any, accum: Any, lr: Any, grad: Any, use_locking: bool = ..., update_slots: bool = ..., name: Optional[Any] = ...): ...

ApplyAdagrad: Any

def apply_adagrad_eager_fallback(var: Any, accum: Any, lr: Any, grad: Any, use_locking: Any, update_slots: Any, name: Any, ctx: Any) -> None: ...
def apply_adagrad_da(var: Any, gradient_accumulator: Any, gradient_squared_accumulator: Any, grad: Any, lr: Any, l1: Any, l2: Any, global_step: Any, use_locking: bool = ..., name: Optional[Any] = ...): ...

ApplyAdagradDA: Any

def apply_adagrad_da_eager_fallback(var: Any, gradient_accumulator: Any, gradient_squared_accumulator: Any, grad: Any, lr: Any, l1: Any, l2: Any, global_step: Any, use_locking: Any, name: Any, ctx: Any) -> None: ...
def apply_adagrad_v2(var: Any, accum: Any, lr: Any, epsilon: Any, grad: Any, use_locking: bool = ..., update_slots: bool = ..., name: Optional[Any] = ...): ...

ApplyAdagradV2: Any

def apply_adagrad_v2_eager_fallback(var: Any, accum: Any, lr: Any, epsilon: Any, grad: Any, use_locking: Any, update_slots: Any, name: Any, ctx: Any) -> None: ...
def apply_adam(var: Any, m: Any, v: Any, beta1_power: Any, beta2_power: Any, lr: Any, beta1: Any, beta2: Any, epsilon: Any, grad: Any, use_locking: bool = ..., use_nesterov: bool = ..., name: Optional[Any] = ...): ...

ApplyAdam: Any

def apply_adam_eager_fallback(var: Any, m: Any, v: Any, beta1_power: Any, beta2_power: Any, lr: Any, beta1: Any, beta2: Any, epsilon: Any, grad: Any, use_locking: Any, use_nesterov: Any, name: Any, ctx: Any) -> None: ...
def apply_add_sign(var: Any, m: Any, lr: Any, alpha: Any, sign_decay: Any, beta: Any, grad: Any, use_locking: bool = ..., name: Optional[Any] = ...): ...

ApplyAddSign: Any

def apply_add_sign_eager_fallback(var: Any, m: Any, lr: Any, alpha: Any, sign_decay: Any, beta: Any, grad: Any, use_locking: Any, name: Any, ctx: Any) -> None: ...
def apply_centered_rms_prop(var: Any, mg: Any, ms: Any, mom: Any, lr: Any, rho: Any, momentum: Any, epsilon: Any, grad: Any, use_locking: bool = ..., name: Optional[Any] = ...): ...

ApplyCenteredRMSProp: Any

def apply_centered_rms_prop_eager_fallback(var: Any, mg: Any, ms: Any, mom: Any, lr: Any, rho: Any, momentum: Any, epsilon: Any, grad: Any, use_locking: Any, name: Any, ctx: Any) -> None: ...
def apply_ftrl(var: Any, accum: Any, linear: Any, grad: Any, lr: Any, l1: Any, l2: Any, lr_power: Any, use_locking: bool = ..., name: Optional[Any] = ...): ...

ApplyFtrl: Any

def apply_ftrl_eager_fallback(var: Any, accum: Any, linear: Any, grad: Any, lr: Any, l1: Any, l2: Any, lr_power: Any, use_locking: Any, name: Any, ctx: Any) -> None: ...
def apply_ftrl_v2(var: Any, accum: Any, linear: Any, grad: Any, lr: Any, l1: Any, l2: Any, l2_shrinkage: Any, lr_power: Any, use_locking: bool = ..., name: Optional[Any] = ...): ...

ApplyFtrlV2: Any

def apply_ftrl_v2_eager_fallback(var: Any, accum: Any, linear: Any, grad: Any, lr: Any, l1: Any, l2: Any, l2_shrinkage: Any, lr_power: Any, use_locking: Any, name: Any, ctx: Any) -> None: ...
def apply_gradient_descent(var: Any, alpha: Any, delta: Any, use_locking: bool = ..., name: Optional[Any] = ...): ...

ApplyGradientDescent: Any

def apply_gradient_descent_eager_fallback(var: Any, alpha: Any, delta: Any, use_locking: Any, name: Any, ctx: Any) -> None: ...
def apply_momentum(var: Any, accum: Any, lr: Any, grad: Any, momentum: Any, use_locking: bool = ..., use_nesterov: bool = ..., name: Optional[Any] = ...): ...

ApplyMomentum: Any

def apply_momentum_eager_fallback(var: Any, accum: Any, lr: Any, grad: Any, momentum: Any, use_locking: Any, use_nesterov: Any, name: Any, ctx: Any) -> None: ...
def apply_power_sign(var: Any, m: Any, lr: Any, logbase: Any, sign_decay: Any, beta: Any, grad: Any, use_locking: bool = ..., name: Optional[Any] = ...): ...

ApplyPowerSign: Any

def apply_power_sign_eager_fallback(var: Any, m: Any, lr: Any, logbase: Any, sign_decay: Any, beta: Any, grad: Any, use_locking: Any, name: Any, ctx: Any) -> None: ...
def apply_proximal_adagrad(var: Any, accum: Any, lr: Any, l1: Any, l2: Any, grad: Any, use_locking: bool = ..., name: Optional[Any] = ...): ...

ApplyProximalAdagrad: Any

def apply_proximal_adagrad_eager_fallback(var: Any, accum: Any, lr: Any, l1: Any, l2: Any, grad: Any, use_locking: Any, name: Any, ctx: Any) -> None: ...
def apply_proximal_gradient_descent(var: Any, alpha: Any, l1: Any, l2: Any, delta: Any, use_locking: bool = ..., name: Optional[Any] = ...): ...

ApplyProximalGradientDescent: Any

def apply_proximal_gradient_descent_eager_fallback(var: Any, alpha: Any, l1: Any, l2: Any, delta: Any, use_locking: Any, name: Any, ctx: Any) -> None: ...
def apply_rms_prop(var: Any, ms: Any, mom: Any, lr: Any, rho: Any, momentum: Any, epsilon: Any, grad: Any, use_locking: bool = ..., name: Optional[Any] = ...): ...

ApplyRMSProp: Any

def apply_rms_prop_eager_fallback(var: Any, ms: Any, mom: Any, lr: Any, rho: Any, momentum: Any, epsilon: Any, grad: Any, use_locking: Any, name: Any, ctx: Any) -> None: ...
def resource_apply_ada_max(var: Any, m: Any, v: Any, beta1_power: Any, lr: Any, beta1: Any, beta2: Any, epsilon: Any, grad: Any, use_locking: bool = ..., name: Optional[Any] = ...): ...

ResourceApplyAdaMax: Any

def resource_apply_ada_max_eager_fallback(var: Any, m: Any, v: Any, beta1_power: Any, lr: Any, beta1: Any, beta2: Any, epsilon: Any, grad: Any, use_locking: Any, name: Any, ctx: Any): ...
def resource_apply_adadelta(var: Any, accum: Any, accum_update: Any, lr: Any, rho: Any, epsilon: Any, grad: Any, use_locking: bool = ..., name: Optional[Any] = ...): ...

ResourceApplyAdadelta: Any

def resource_apply_adadelta_eager_fallback(var: Any, accum: Any, accum_update: Any, lr: Any, rho: Any, epsilon: Any, grad: Any, use_locking: Any, name: Any, ctx: Any): ...
def resource_apply_adagrad(var: Any, accum: Any, lr: Any, grad: Any, use_locking: bool = ..., update_slots: bool = ..., name: Optional[Any] = ...): ...

ResourceApplyAdagrad: Any

def resource_apply_adagrad_eager_fallback(var: Any, accum: Any, lr: Any, grad: Any, use_locking: Any, update_slots: Any, name: Any, ctx: Any): ...
def resource_apply_adagrad_da(var: Any, gradient_accumulator: Any, gradient_squared_accumulator: Any, grad: Any, lr: Any, l1: Any, l2: Any, global_step: Any, use_locking: bool = ..., name: Optional[Any] = ...): ...

ResourceApplyAdagradDA: Any

def resource_apply_adagrad_da_eager_fallback(var: Any, gradient_accumulator: Any, gradient_squared_accumulator: Any, grad: Any, lr: Any, l1: Any, l2: Any, global_step: Any, use_locking: Any, name: Any, ctx: Any): ...
def resource_apply_adagrad_v2(var: Any, accum: Any, lr: Any, epsilon: Any, grad: Any, use_locking: bool = ..., update_slots: bool = ..., name: Optional[Any] = ...): ...

ResourceApplyAdagradV2: Any

def resource_apply_adagrad_v2_eager_fallback(var: Any, accum: Any, lr: Any, epsilon: Any, grad: Any, use_locking: Any, update_slots: Any, name: Any, ctx: Any): ...
def resource_apply_adam(var: Any, m: Any, v: Any, beta1_power: Any, beta2_power: Any, lr: Any, beta1: Any, beta2: Any, epsilon: Any, grad: Any, use_locking: bool = ..., use_nesterov: bool = ..., name: Optional[Any] = ...): ...

ResourceApplyAdam: Any

def resource_apply_adam_eager_fallback(var: Any, m: Any, v: Any, beta1_power: Any, beta2_power: Any, lr: Any, beta1: Any, beta2: Any, epsilon: Any, grad: Any, use_locking: Any, use_nesterov: Any, name: Any, ctx: Any): ...
def resource_apply_adam_with_amsgrad(var: Any, m: Any, v: Any, vhat: Any, beta1_power: Any, beta2_power: Any, lr: Any, beta1: Any, beta2: Any, epsilon: Any, grad: Any, use_locking: bool = ..., name: Optional[Any] = ...): ...

ResourceApplyAdamWithAmsgrad: Any

def resource_apply_adam_with_amsgrad_eager_fallback(var: Any, m: Any, v: Any, vhat: Any, beta1_power: Any, beta2_power: Any, lr: Any, beta1: Any, beta2: Any, epsilon: Any, grad: Any, use_locking: Any, name: Any, ctx: Any): ...
def resource_apply_add_sign(var: Any, m: Any, lr: Any, alpha: Any, sign_decay: Any, beta: Any, grad: Any, use_locking: bool = ..., name: Optional[Any] = ...): ...

ResourceApplyAddSign: Any

def resource_apply_add_sign_eager_fallback(var: Any, m: Any, lr: Any, alpha: Any, sign_decay: Any, beta: Any, grad: Any, use_locking: Any, name: Any, ctx: Any): ...
def resource_apply_centered_rms_prop(var: Any, mg: Any, ms: Any, mom: Any, lr: Any, rho: Any, momentum: Any, epsilon: Any, grad: Any, use_locking: bool = ..., name: Optional[Any] = ...): ...

ResourceApplyCenteredRMSProp: Any

def resource_apply_centered_rms_prop_eager_fallback(var: Any, mg: Any, ms: Any, mom: Any, lr: Any, rho: Any, momentum: Any, epsilon: Any, grad: Any, use_locking: Any, name: Any, ctx: Any): ...
def resource_apply_ftrl(var: Any, accum: Any, linear: Any, grad: Any, lr: Any, l1: Any, l2: Any, lr_power: Any, use_locking: bool = ..., name: Optional[Any] = ...): ...

ResourceApplyFtrl: Any

def resource_apply_ftrl_eager_fallback(var: Any, accum: Any, linear: Any, grad: Any, lr: Any, l1: Any, l2: Any, lr_power: Any, use_locking: Any, name: Any, ctx: Any): ...
def resource_apply_ftrl_v2(var: Any, accum: Any, linear: Any, grad: Any, lr: Any, l1: Any, l2: Any, l2_shrinkage: Any, lr_power: Any, use_locking: bool = ..., name: Optional[Any] = ...): ...

ResourceApplyFtrlV2: Any

def resource_apply_ftrl_v2_eager_fallback(var: Any, accum: Any, linear: Any, grad: Any, lr: Any, l1: Any, l2: Any, l2_shrinkage: Any, lr_power: Any, use_locking: Any, name: Any, ctx: Any): ...
def resource_apply_gradient_descent(var: Any, alpha: Any, delta: Any, use_locking: bool = ..., name: Optional[Any] = ...): ...

ResourceApplyGradientDescent: Any

def resource_apply_gradient_descent_eager_fallback(var: Any, alpha: Any, delta: Any, use_locking: Any, name: Any, ctx: Any): ...
def resource_apply_keras_momentum(var: Any, accum: Any, lr: Any, grad: Any, momentum: Any, use_locking: bool = ..., use_nesterov: bool = ..., name: Optional[Any] = ...): ...

ResourceApplyKerasMomentum: Any

def resource_apply_keras_momentum_eager_fallback(var: Any, accum: Any, lr: Any, grad: Any, momentum: Any, use_locking: Any, use_nesterov: Any, name: Any, ctx: Any): ...
def resource_apply_momentum(var: Any, accum: Any, lr: Any, grad: Any, momentum: Any, use_locking: bool = ..., use_nesterov: bool = ..., name: Optional[Any] = ...): ...

ResourceApplyMomentum: Any

def resource_apply_momentum_eager_fallback(var: Any, accum: Any, lr: Any, grad: Any, momentum: Any, use_locking: Any, use_nesterov: Any, name: Any, ctx: Any): ...
def resource_apply_power_sign(var: Any, m: Any, lr: Any, logbase: Any, sign_decay: Any, beta: Any, grad: Any, use_locking: bool = ..., name: Optional[Any] = ...): ...

ResourceApplyPowerSign: Any

def resource_apply_power_sign_eager_fallback(var: Any, m: Any, lr: Any, logbase: Any, sign_decay: Any, beta: Any, grad: Any, use_locking: Any, name: Any, ctx: Any): ...
def resource_apply_proximal_adagrad(var: Any, accum: Any, lr: Any, l1: Any, l2: Any, grad: Any, use_locking: bool = ..., name: Optional[Any] = ...): ...

ResourceApplyProximalAdagrad: Any

def resource_apply_proximal_adagrad_eager_fallback(var: Any, accum: Any, lr: Any, l1: Any, l2: Any, grad: Any, use_locking: Any, name: Any, ctx: Any): ...
def resource_apply_proximal_gradient_descent(var: Any, alpha: Any, l1: Any, l2: Any, delta: Any, use_locking: bool = ..., name: Optional[Any] = ...): ...

ResourceApplyProximalGradientDescent: Any

def resource_apply_proximal_gradient_descent_eager_fallback(var: Any, alpha: Any, l1: Any, l2: Any, delta: Any, use_locking: Any, name: Any, ctx: Any): ...
def resource_apply_rms_prop(var: Any, ms: Any, mom: Any, lr: Any, rho: Any, momentum: Any, epsilon: Any, grad: Any, use_locking: bool = ..., name: Optional[Any] = ...): ...

ResourceApplyRMSProp: Any

def resource_apply_rms_prop_eager_fallback(var: Any, ms: Any, mom: Any, lr: Any, rho: Any, momentum: Any, epsilon: Any, grad: Any, use_locking: Any, name: Any, ctx: Any): ...
def resource_sparse_apply_adadelta(var: Any, accum: Any, accum_update: Any, lr: Any, rho: Any, epsilon: Any, grad: Any, indices: Any, use_locking: bool = ..., name: Optional[Any] = ...): ...

ResourceSparseApplyAdadelta: Any

def resource_sparse_apply_adadelta_eager_fallback(var: Any, accum: Any, accum_update: Any, lr: Any, rho: Any, epsilon: Any, grad: Any, indices: Any, use_locking: Any, name: Any, ctx: Any): ...
def resource_sparse_apply_adagrad(var: Any, accum: Any, lr: Any, grad: Any, indices: Any, use_locking: bool = ..., update_slots: bool = ..., name: Optional[Any] = ...): ...

ResourceSparseApplyAdagrad: Any

def resource_sparse_apply_adagrad_eager_fallback(var: Any, accum: Any, lr: Any, grad: Any, indices: Any, use_locking: Any, update_slots: Any, name: Any, ctx: Any): ...
def resource_sparse_apply_adagrad_da(var: Any, gradient_accumulator: Any, gradient_squared_accumulator: Any, grad: Any, indices: Any, lr: Any, l1: Any, l2: Any, global_step: Any, use_locking: bool = ..., name: Optional[Any] = ...): ...

ResourceSparseApplyAdagradDA: Any

def resource_sparse_apply_adagrad_da_eager_fallback(var: Any, gradient_accumulator: Any, gradient_squared_accumulator: Any, grad: Any, indices: Any, lr: Any, l1: Any, l2: Any, global_step: Any, use_locking: Any, name: Any, ctx: Any): ...
def resource_sparse_apply_adagrad_v2(var: Any, accum: Any, lr: Any, epsilon: Any, grad: Any, indices: Any, use_locking: bool = ..., update_slots: bool = ..., name: Optional[Any] = ...): ...

ResourceSparseApplyAdagradV2: Any

def resource_sparse_apply_adagrad_v2_eager_fallback(var: Any, accum: Any, lr: Any, epsilon: Any, grad: Any, indices: Any, use_locking: Any, update_slots: Any, name: Any, ctx: Any): ...
def resource_sparse_apply_centered_rms_prop(var: Any, mg: Any, ms: Any, mom: Any, lr: Any, rho: Any, momentum: Any, epsilon: Any, grad: Any, indices: Any, use_locking: bool = ..., name: Optional[Any] = ...): ...

ResourceSparseApplyCenteredRMSProp: Any

def resource_sparse_apply_centered_rms_prop_eager_fallback(var: Any, mg: Any, ms: Any, mom: Any, lr: Any, rho: Any, momentum: Any, epsilon: Any, grad: Any, indices: Any, use_locking: Any, name: Any, ctx: Any): ...
def resource_sparse_apply_ftrl(var: Any, accum: Any, linear: Any, grad: Any, indices: Any, lr: Any, l1: Any, l2: Any, lr_power: Any, use_locking: bool = ..., name: Optional[Any] = ...): ...

ResourceSparseApplyFtrl: Any

def resource_sparse_apply_ftrl_eager_fallback(var: Any, accum: Any, linear: Any, grad: Any, indices: Any, lr: Any, l1: Any, l2: Any, lr_power: Any, use_locking: Any, name: Any, ctx: Any): ...
def resource_sparse_apply_ftrl_v2(var: Any, accum: Any, linear: Any, grad: Any, indices: Any, lr: Any, l1: Any, l2: Any, l2_shrinkage: Any, lr_power: Any, use_locking: bool = ..., name: Optional[Any] = ...): ...

ResourceSparseApplyFtrlV2: Any

def resource_sparse_apply_ftrl_v2_eager_fallback(var: Any, accum: Any, linear: Any, grad: Any, indices: Any, lr: Any, l1: Any, l2: Any, l2_shrinkage: Any, lr_power: Any, use_locking: Any, name: Any, ctx: Any): ...
def resource_sparse_apply_keras_momentum(var: Any, accum: Any, lr: Any, grad: Any, indices: Any, momentum: Any, use_locking: bool = ..., use_nesterov: bool = ..., name: Optional[Any] = ...): ...

ResourceSparseApplyKerasMomentum: Any

def resource_sparse_apply_keras_momentum_eager_fallback(var: Any, accum: Any, lr: Any, grad: Any, indices: Any, momentum: Any, use_locking: Any, use_nesterov: Any, name: Any, ctx: Any): ...
def resource_sparse_apply_momentum(var: Any, accum: Any, lr: Any, grad: Any, indices: Any, momentum: Any, use_locking: bool = ..., use_nesterov: bool = ..., name: Optional[Any] = ...): ...

ResourceSparseApplyMomentum: Any

def resource_sparse_apply_momentum_eager_fallback(var: Any, accum: Any, lr: Any, grad: Any, indices: Any, momentum: Any, use_locking: Any, use_nesterov: Any, name: Any, ctx: Any): ...
def resource_sparse_apply_proximal_adagrad(var: Any, accum: Any, lr: Any, l1: Any, l2: Any, grad: Any, indices: Any, use_locking: bool = ..., name: Optional[Any] = ...): ...

ResourceSparseApplyProximalAdagrad: Any

def resource_sparse_apply_proximal_adagrad_eager_fallback(var: Any, accum: Any, lr: Any, l1: Any, l2: Any, grad: Any, indices: Any, use_locking: Any, name: Any, ctx: Any): ...
def resource_sparse_apply_proximal_gradient_descent(var: Any, alpha: Any, l1: Any, l2: Any, grad: Any, indices: Any, use_locking: bool = ..., name: Optional[Any] = ...): ...

ResourceSparseApplyProximalGradientDescent: Any

def resource_sparse_apply_proximal_gradient_descent_eager_fallback(var: Any, alpha: Any, l1: Any, l2: Any, grad: Any, indices: Any, use_locking: Any, name: Any, ctx: Any): ...
def resource_sparse_apply_rms_prop(var: Any, ms: Any, mom: Any, lr: Any, rho: Any, momentum: Any, epsilon: Any, grad: Any, indices: Any, use_locking: bool = ..., name: Optional[Any] = ...): ...

ResourceSparseApplyRMSProp: Any

def resource_sparse_apply_rms_prop_eager_fallback(var: Any, ms: Any, mom: Any, lr: Any, rho: Any, momentum: Any, epsilon: Any, grad: Any, indices: Any, use_locking: Any, name: Any, ctx: Any): ...
def sparse_apply_adadelta(var: Any, accum: Any, accum_update: Any, lr: Any, rho: Any, epsilon: Any, grad: Any, indices: Any, use_locking: bool = ..., name: Optional[Any] = ...): ...

SparseApplyAdadelta: Any

def sparse_apply_adadelta_eager_fallback(var: Any, accum: Any, accum_update: Any, lr: Any, rho: Any, epsilon: Any, grad: Any, indices: Any, use_locking: Any, name: Any, ctx: Any) -> None: ...
def sparse_apply_adagrad(var: Any, accum: Any, lr: Any, grad: Any, indices: Any, use_locking: bool = ..., update_slots: bool = ..., name: Optional[Any] = ...): ...

SparseApplyAdagrad: Any

def sparse_apply_adagrad_eager_fallback(var: Any, accum: Any, lr: Any, grad: Any, indices: Any, use_locking: Any, update_slots: Any, name: Any, ctx: Any) -> None: ...
def sparse_apply_adagrad_da(var: Any, gradient_accumulator: Any, gradient_squared_accumulator: Any, grad: Any, indices: Any, lr: Any, l1: Any, l2: Any, global_step: Any, use_locking: bool = ..., name: Optional[Any] = ...): ...

SparseApplyAdagradDA: Any

def sparse_apply_adagrad_da_eager_fallback(var: Any, gradient_accumulator: Any, gradient_squared_accumulator: Any, grad: Any, indices: Any, lr: Any, l1: Any, l2: Any, global_step: Any, use_locking: Any, name: Any, ctx: Any) -> None: ...
def sparse_apply_adagrad_v2(var: Any, accum: Any, lr: Any, epsilon: Any, grad: Any, indices: Any, use_locking: bool = ..., update_slots: bool = ..., name: Optional[Any] = ...): ...

SparseApplyAdagradV2: Any

def sparse_apply_adagrad_v2_eager_fallback(var: Any, accum: Any, lr: Any, epsilon: Any, grad: Any, indices: Any, use_locking: Any, update_slots: Any, name: Any, ctx: Any) -> None: ...
def sparse_apply_centered_rms_prop(var: Any, mg: Any, ms: Any, mom: Any, lr: Any, rho: Any, momentum: Any, epsilon: Any, grad: Any, indices: Any, use_locking: bool = ..., name: Optional[Any] = ...): ...

SparseApplyCenteredRMSProp: Any

def sparse_apply_centered_rms_prop_eager_fallback(var: Any, mg: Any, ms: Any, mom: Any, lr: Any, rho: Any, momentum: Any, epsilon: Any, grad: Any, indices: Any, use_locking: Any, name: Any, ctx: Any) -> None: ...
def sparse_apply_ftrl(var: Any, accum: Any, linear: Any, grad: Any, indices: Any, lr: Any, l1: Any, l2: Any, lr_power: Any, use_locking: bool = ..., name: Optional[Any] = ...): ...

SparseApplyFtrl: Any

def sparse_apply_ftrl_eager_fallback(var: Any, accum: Any, linear: Any, grad: Any, indices: Any, lr: Any, l1: Any, l2: Any, lr_power: Any, use_locking: Any, name: Any, ctx: Any) -> None: ...
def sparse_apply_ftrl_v2(var: Any, accum: Any, linear: Any, grad: Any, indices: Any, lr: Any, l1: Any, l2: Any, l2_shrinkage: Any, lr_power: Any, use_locking: bool = ..., name: Optional[Any] = ...): ...

SparseApplyFtrlV2: Any

def sparse_apply_ftrl_v2_eager_fallback(var: Any, accum: Any, linear: Any, grad: Any, indices: Any, lr: Any, l1: Any, l2: Any, l2_shrinkage: Any, lr_power: Any, use_locking: Any, name: Any, ctx: Any) -> None: ...
def sparse_apply_momentum(var: Any, accum: Any, lr: Any, grad: Any, indices: Any, momentum: Any, use_locking: bool = ..., use_nesterov: bool = ..., name: Optional[Any] = ...): ...

SparseApplyMomentum: Any

def sparse_apply_momentum_eager_fallback(var: Any, accum: Any, lr: Any, grad: Any, indices: Any, momentum: Any, use_locking: Any, use_nesterov: Any, name: Any, ctx: Any) -> None: ...
def sparse_apply_proximal_adagrad(var: Any, accum: Any, lr: Any, l1: Any, l2: Any, grad: Any, indices: Any, use_locking: bool = ..., name: Optional[Any] = ...): ...

SparseApplyProximalAdagrad: Any

def sparse_apply_proximal_adagrad_eager_fallback(var: Any, accum: Any, lr: Any, l1: Any, l2: Any, grad: Any, indices: Any, use_locking: Any, name: Any, ctx: Any) -> None: ...
def sparse_apply_proximal_gradient_descent(var: Any, alpha: Any, l1: Any, l2: Any, grad: Any, indices: Any, use_locking: bool = ..., name: Optional[Any] = ...): ...

SparseApplyProximalGradientDescent: Any

def sparse_apply_proximal_gradient_descent_eager_fallback(var: Any, alpha: Any, l1: Any, l2: Any, grad: Any, indices: Any, use_locking: Any, name: Any, ctx: Any) -> None: ...
def sparse_apply_rms_prop(var: Any, ms: Any, mom: Any, lr: Any, rho: Any, momentum: Any, epsilon: Any, grad: Any, indices: Any, use_locking: bool = ..., name: Optional[Any] = ...): ...

SparseApplyRMSProp: Any

def sparse_apply_rms_prop_eager_fallback(var: Any, ms: Any, mom: Any, lr: Any, rho: Any, momentum: Any, epsilon: Any, grad: Any, indices: Any, use_locking: Any, name: Any, ctx: Any) -> None: ...
