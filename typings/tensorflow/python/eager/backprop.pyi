from tensorflow.python import pywrap_tfe as pywrap_tfe
from tensorflow.python.eager import backprop_util as backprop_util, context as context, execute as execute, imperative_grad as imperative_grad, tape as tape
from tensorflow.python.framework import constant_op as constant_op, dtypes as dtypes, ops as ops, tensor_shape as tensor_shape, tensor_util as tensor_util
from tensorflow.python.ops import array_ops as array_ops, check_ops as check_ops, control_flow_util as control_flow_util, default_gradient as default_gradient, gen_array_ops as gen_array_ops, gen_math_ops as gen_math_ops, math_ops as math_ops, resource_variable_ops as resource_variable_ops
from tensorflow.python.ops.unconnected_gradients import UnconnectedGradients as UnconnectedGradients
from tensorflow.python.util import nest as nest, tf_contextlib as tf_contextlib, tf_inspect as tf_inspect
from tensorflow.python.util.lazy_loader import LazyLoader as LazyLoader
from tensorflow.python.util.tf_export import tf_export as tf_export
from typing import Any, Optional

pfor_ops: Any
function: Any

def op_attr_type(op_type: Any, attr_name: Any): ...
def make_attr(attr_type: Any, value: Any): ...

class _MockOp:
    attrs: Any = ...
    inputs: Any = ...
    outputs: Any = ...
    type: Any = ...
    skip_input_indices: Any = ...
    def __init__(self, attrs: Any, inputs: Any, outputs: Any, typ: Any, skip_input_indices: Any) -> None: ...
    def get_attr(self, attr: Any): ...

def implicit_val_and_grad(f: Any): ...
def implicit_grad(f: Any): ...
def gradients_function(f: Any, params: Optional[Any] = ...): ...
def val_and_grad_function(f: Any, params: Optional[Any] = ...): ...
def make_vjp(f: Any, params: Optional[Any] = ..., persistent: bool = ...): ...
def flatten_nested_indexed_slices(grad: Any): ...
def aggregate_indexed_slices_gradients(grads: Any): ...

class GradientTape:
    def __init__(self, persistent: bool = ..., watch_accessed_variables: bool = ...) -> None: ...
    def __enter__(self): ...
    def __exit__(self, typ: Any, value: Any, traceback: Any) -> None: ...
    def __del__(self) -> None: ...
    def watch(self, tensor: Any) -> None: ...
    def stop_recording(self) -> None: ...
    def reset(self) -> None: ...
    def watched_variables(self): ...
    def gradient(self, target: Any, sources: Any, output_gradients: Optional[Any] = ..., unconnected_gradients: Any = ...): ...
    def jacobian(self, target: Any, sources: Any, unconnected_gradients: Any = ..., parallel_iterations: Optional[Any] = ..., experimental_use_pfor: bool = ...): ...
    def batch_jacobian(self, target: Any, source: Any, unconnected_gradients: Any = ..., parallel_iterations: Optional[Any] = ..., experimental_use_pfor: bool = ...): ...
