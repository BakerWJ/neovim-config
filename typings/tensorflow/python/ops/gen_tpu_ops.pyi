from collections import namedtuple
from tensorflow.python.util.deprecation import deprecated_endpoints as deprecated_endpoints
from tensorflow.python.util.tf_export import tf_export as tf_export
from typing import Any, Optional

def all_to_all(input: Any, group_assignment: Any, concat_dimension: Any, split_dimension: Any, split_count: Any, name: Optional[Any] = ...): ...

AllToAll: Any

def all_to_all_eager_fallback(input: Any, group_assignment: Any, concat_dimension: Any, split_dimension: Any, split_count: Any, name: Any, ctx: Any): ...
def collective_permute(input: Any, source_target_pairs: Any, name: Optional[Any] = ...): ...

CollectivePermute: Any

def collective_permute_eager_fallback(input: Any, source_target_pairs: Any, name: Any, ctx: Any): ...
def configure_distributed_tpu(embedding_config: str = ..., tpu_embedding_config: str = ..., is_global_init: bool = ..., enable_whole_mesh_compilations: bool = ..., compilation_failure_closes_chips: bool = ..., name: Optional[Any] = ...): ...

ConfigureDistributedTPU: Any

def configure_distributed_tpu_eager_fallback(embedding_config: Any, tpu_embedding_config: Any, is_global_init: Any, enable_whole_mesh_compilations: Any, compilation_failure_closes_chips: Any, name: Any, ctx: Any): ...
def configure_tpu_embedding(config: Any, name: Optional[Any] = ...): ...

ConfigureTPUEmbedding: Any

def configure_tpu_embedding_eager_fallback(config: Any, name: Any, ctx: Any): ...
def cross_replica_sum(input: Any, group_assignment: Any, name: Optional[Any] = ...): ...

CrossReplicaSum: Any

def cross_replica_sum_eager_fallback(input: Any, group_assignment: Any, name: Any, ctx: Any): ...
def enqueue_tpu_embedding_integer_batch(batch: Any, mode_override: Any, device_ordinal: int = ..., name: Optional[Any] = ...): ...

EnqueueTPUEmbeddingIntegerBatch: Any

def enqueue_tpu_embedding_integer_batch_eager_fallback(batch: Any, mode_override: Any, device_ordinal: Any, name: Any, ctx: Any): ...
def enqueue_tpu_embedding_sparse_batch(sample_indices: Any, embedding_indices: Any, aggregation_weights: Any, mode_override: Any, device_ordinal: int = ..., combiners: Any = ..., name: Optional[Any] = ...): ...

EnqueueTPUEmbeddingSparseBatch: Any

def enqueue_tpu_embedding_sparse_batch_eager_fallback(sample_indices: Any, embedding_indices: Any, aggregation_weights: Any, mode_override: Any, device_ordinal: Any, combiners: Any, name: Any, ctx: Any): ...
def enqueue_tpu_embedding_sparse_tensor_batch(sample_indices: Any, embedding_indices: Any, aggregation_weights: Any, mode_override: Any, table_ids: Any, device_ordinal: int = ..., combiners: Any = ..., max_sequence_lengths: Any = ..., name: Optional[Any] = ...): ...

EnqueueTPUEmbeddingSparseTensorBatch: Any

def enqueue_tpu_embedding_sparse_tensor_batch_eager_fallback(sample_indices: Any, embedding_indices: Any, aggregation_weights: Any, mode_override: Any, table_ids: Any, device_ordinal: Any, combiners: Any, max_sequence_lengths: Any, name: Any, ctx: Any): ...
def infeed_dequeue(dtype: Any, shape: Any, name: Optional[Any] = ...): ...

InfeedDequeue: Any

def infeed_dequeue_eager_fallback(dtype: Any, shape: Any, name: Any, ctx: Any): ...
def infeed_dequeue_tuple(dtypes: Any, shapes: Any, name: Optional[Any] = ...): ...

InfeedDequeueTuple: Any

def infeed_dequeue_tuple_eager_fallback(dtypes: Any, shapes: Any, name: Any, ctx: Any): ...
def infeed_enqueue(input: Any, shape: Any = ..., layout: Any = ..., device_ordinal: int = ..., name: Optional[Any] = ...): ...

InfeedEnqueue: Any

def infeed_enqueue_eager_fallback(input: Any, shape: Any, layout: Any, device_ordinal: Any, name: Any, ctx: Any): ...
def infeed_enqueue_prelinearized_buffer(input: Any, device_ordinal: int = ..., name: Optional[Any] = ...): ...

InfeedEnqueuePrelinearizedBuffer: Any

def infeed_enqueue_prelinearized_buffer_eager_fallback(input: Any, device_ordinal: Any, name: Any, ctx: Any): ...
def infeed_enqueue_tuple(inputs: Any, shapes: Any, layouts: Any = ..., device_ordinal: int = ..., name: Optional[Any] = ...): ...

InfeedEnqueueTuple: Any

def infeed_enqueue_tuple_eager_fallback(inputs: Any, shapes: Any, layouts: Any, device_ordinal: Any, name: Any, ctx: Any): ...
def load_tpu_embedding_adam_parameters(parameters: Any, momenta: Any, velocities: Any, num_shards: Any, shard_id: Any, table_id: int = ..., table_name: str = ..., config: str = ..., name: Optional[Any] = ...): ...

LoadTPUEmbeddingADAMParameters: Any

def load_tpu_embedding_adam_parameters_eager_fallback(parameters: Any, momenta: Any, velocities: Any, num_shards: Any, shard_id: Any, table_id: Any, table_name: Any, config: Any, name: Any, ctx: Any): ...
def load_tpu_embedding_adam_parameters_grad_accum_debug(parameters: Any, momenta: Any, velocities: Any, gradient_accumulators: Any, num_shards: Any, shard_id: Any, table_id: int = ..., table_name: str = ..., config: str = ..., name: Optional[Any] = ...): ...

LoadTPUEmbeddingADAMParametersGradAccumDebug: Any

def load_tpu_embedding_adam_parameters_grad_accum_debug_eager_fallback(parameters: Any, momenta: Any, velocities: Any, gradient_accumulators: Any, num_shards: Any, shard_id: Any, table_id: Any, table_name: Any, config: Any, name: Any, ctx: Any): ...
def load_tpu_embedding_adadelta_parameters(parameters: Any, accumulators: Any, updates: Any, num_shards: Any, shard_id: Any, table_id: int = ..., table_name: str = ..., config: str = ..., name: Optional[Any] = ...): ...

LoadTPUEmbeddingAdadeltaParameters: Any

def load_tpu_embedding_adadelta_parameters_eager_fallback(parameters: Any, accumulators: Any, updates: Any, num_shards: Any, shard_id: Any, table_id: Any, table_name: Any, config: Any, name: Any, ctx: Any): ...
def load_tpu_embedding_adadelta_parameters_grad_accum_debug(parameters: Any, accumulators: Any, updates: Any, gradient_accumulators: Any, num_shards: Any, shard_id: Any, table_id: int = ..., table_name: str = ..., config: str = ..., name: Optional[Any] = ...): ...

LoadTPUEmbeddingAdadeltaParametersGradAccumDebug: Any

def load_tpu_embedding_adadelta_parameters_grad_accum_debug_eager_fallback(parameters: Any, accumulators: Any, updates: Any, gradient_accumulators: Any, num_shards: Any, shard_id: Any, table_id: Any, table_name: Any, config: Any, name: Any, ctx: Any): ...
def load_tpu_embedding_adagrad_parameters(parameters: Any, accumulators: Any, num_shards: Any, shard_id: Any, table_id: int = ..., table_name: str = ..., config: str = ..., name: Optional[Any] = ...): ...

LoadTPUEmbeddingAdagradParameters: Any

def load_tpu_embedding_adagrad_parameters_eager_fallback(parameters: Any, accumulators: Any, num_shards: Any, shard_id: Any, table_id: Any, table_name: Any, config: Any, name: Any, ctx: Any): ...
def load_tpu_embedding_adagrad_parameters_grad_accum_debug(parameters: Any, accumulators: Any, gradient_accumulators: Any, num_shards: Any, shard_id: Any, table_id: int = ..., table_name: str = ..., config: str = ..., name: Optional[Any] = ...): ...

LoadTPUEmbeddingAdagradParametersGradAccumDebug: Any

def load_tpu_embedding_adagrad_parameters_grad_accum_debug_eager_fallback(parameters: Any, accumulators: Any, gradient_accumulators: Any, num_shards: Any, shard_id: Any, table_id: Any, table_name: Any, config: Any, name: Any, ctx: Any): ...
def load_tpu_embedding_centered_rms_prop_parameters(parameters: Any, ms: Any, mom: Any, mg: Any, num_shards: Any, shard_id: Any, table_id: int = ..., table_name: str = ..., config: str = ..., name: Optional[Any] = ...): ...

LoadTPUEmbeddingCenteredRMSPropParameters: Any

def load_tpu_embedding_centered_rms_prop_parameters_eager_fallback(parameters: Any, ms: Any, mom: Any, mg: Any, num_shards: Any, shard_id: Any, table_id: Any, table_name: Any, config: Any, name: Any, ctx: Any): ...
def load_tpu_embedding_ftrl_parameters(parameters: Any, accumulators: Any, linears: Any, num_shards: Any, shard_id: Any, table_id: int = ..., table_name: str = ..., config: str = ..., name: Optional[Any] = ...): ...

LoadTPUEmbeddingFTRLParameters: Any

def load_tpu_embedding_ftrl_parameters_eager_fallback(parameters: Any, accumulators: Any, linears: Any, num_shards: Any, shard_id: Any, table_id: Any, table_name: Any, config: Any, name: Any, ctx: Any): ...
def load_tpu_embedding_ftrl_parameters_grad_accum_debug(parameters: Any, accumulators: Any, linears: Any, gradient_accumulators: Any, num_shards: Any, shard_id: Any, table_id: int = ..., table_name: str = ..., config: str = ..., name: Optional[Any] = ...): ...

LoadTPUEmbeddingFTRLParametersGradAccumDebug: Any

def load_tpu_embedding_ftrl_parameters_grad_accum_debug_eager_fallback(parameters: Any, accumulators: Any, linears: Any, gradient_accumulators: Any, num_shards: Any, shard_id: Any, table_id: Any, table_name: Any, config: Any, name: Any, ctx: Any): ...
def load_tpu_embedding_mdl_adagrad_light_parameters(parameters: Any, accumulators: Any, weights: Any, benefits: Any, num_shards: Any, shard_id: Any, table_id: int = ..., table_name: str = ..., config: str = ..., name: Optional[Any] = ...): ...

LoadTPUEmbeddingMDLAdagradLightParameters: Any

def load_tpu_embedding_mdl_adagrad_light_parameters_eager_fallback(parameters: Any, accumulators: Any, weights: Any, benefits: Any, num_shards: Any, shard_id: Any, table_id: Any, table_name: Any, config: Any, name: Any, ctx: Any): ...
def load_tpu_embedding_momentum_parameters(parameters: Any, momenta: Any, num_shards: Any, shard_id: Any, table_id: int = ..., table_name: str = ..., config: str = ..., name: Optional[Any] = ...): ...

LoadTPUEmbeddingMomentumParameters: Any

def load_tpu_embedding_momentum_parameters_eager_fallback(parameters: Any, momenta: Any, num_shards: Any, shard_id: Any, table_id: Any, table_name: Any, config: Any, name: Any, ctx: Any): ...
def load_tpu_embedding_momentum_parameters_grad_accum_debug(parameters: Any, momenta: Any, gradient_accumulators: Any, num_shards: Any, shard_id: Any, table_id: int = ..., table_name: str = ..., config: str = ..., name: Optional[Any] = ...): ...

LoadTPUEmbeddingMomentumParametersGradAccumDebug: Any

def load_tpu_embedding_momentum_parameters_grad_accum_debug_eager_fallback(parameters: Any, momenta: Any, gradient_accumulators: Any, num_shards: Any, shard_id: Any, table_id: Any, table_name: Any, config: Any, name: Any, ctx: Any): ...
def load_tpu_embedding_proximal_adagrad_parameters(parameters: Any, accumulators: Any, num_shards: Any, shard_id: Any, table_id: int = ..., table_name: str = ..., config: str = ..., name: Optional[Any] = ...): ...

LoadTPUEmbeddingProximalAdagradParameters: Any

def load_tpu_embedding_proximal_adagrad_parameters_eager_fallback(parameters: Any, accumulators: Any, num_shards: Any, shard_id: Any, table_id: Any, table_name: Any, config: Any, name: Any, ctx: Any): ...
def load_tpu_embedding_proximal_adagrad_parameters_grad_accum_debug(parameters: Any, accumulators: Any, gradient_accumulators: Any, num_shards: Any, shard_id: Any, table_id: int = ..., table_name: str = ..., config: str = ..., name: Optional[Any] = ...): ...

LoadTPUEmbeddingProximalAdagradParametersGradAccumDebug: Any

def load_tpu_embedding_proximal_adagrad_parameters_grad_accum_debug_eager_fallback(parameters: Any, accumulators: Any, gradient_accumulators: Any, num_shards: Any, shard_id: Any, table_id: Any, table_name: Any, config: Any, name: Any, ctx: Any): ...
def load_tpu_embedding_rms_prop_parameters(parameters: Any, ms: Any, mom: Any, num_shards: Any, shard_id: Any, table_id: int = ..., table_name: str = ..., config: str = ..., name: Optional[Any] = ...): ...

LoadTPUEmbeddingRMSPropParameters: Any

def load_tpu_embedding_rms_prop_parameters_eager_fallback(parameters: Any, ms: Any, mom: Any, num_shards: Any, shard_id: Any, table_id: Any, table_name: Any, config: Any, name: Any, ctx: Any): ...
def load_tpu_embedding_rms_prop_parameters_grad_accum_debug(parameters: Any, ms: Any, mom: Any, gradient_accumulators: Any, num_shards: Any, shard_id: Any, table_id: int = ..., table_name: str = ..., config: str = ..., name: Optional[Any] = ...): ...

LoadTPUEmbeddingRMSPropParametersGradAccumDebug: Any

def load_tpu_embedding_rms_prop_parameters_grad_accum_debug_eager_fallback(parameters: Any, ms: Any, mom: Any, gradient_accumulators: Any, num_shards: Any, shard_id: Any, table_id: Any, table_name: Any, config: Any, name: Any, ctx: Any): ...
def load_tpu_embedding_stochastic_gradient_descent_parameters(parameters: Any, num_shards: Any, shard_id: Any, table_id: int = ..., table_name: str = ..., config: str = ..., name: Optional[Any] = ...): ...

LoadTPUEmbeddingStochasticGradientDescentParameters: Any

def load_tpu_embedding_stochastic_gradient_descent_parameters_eager_fallback(parameters: Any, num_shards: Any, shard_id: Any, table_id: Any, table_name: Any, config: Any, name: Any, ctx: Any): ...
def outfeed_dequeue(dtype: Any, shape: Any, device_ordinal: int = ..., name: Optional[Any] = ...): ...

OutfeedDequeue: Any

def outfeed_dequeue_eager_fallback(dtype: Any, shape: Any, device_ordinal: Any, name: Any, ctx: Any): ...
def outfeed_dequeue_tuple(dtypes: Any, shapes: Any, device_ordinal: int = ..., name: Optional[Any] = ...): ...

OutfeedDequeueTuple: Any

def outfeed_dequeue_tuple_eager_fallback(dtypes: Any, shapes: Any, device_ordinal: Any, name: Any, ctx: Any): ...
def outfeed_enqueue(input: Any, name: Optional[Any] = ...): ...

OutfeedEnqueue: Any

def outfeed_enqueue_eager_fallback(input: Any, name: Any, ctx: Any): ...
def outfeed_enqueue_tuple(inputs: Any, name: Optional[Any] = ...): ...

OutfeedEnqueueTuple: Any

def outfeed_enqueue_tuple_eager_fallback(inputs: Any, name: Any, ctx: Any): ...
def prelinearize(input: Any, shape: Any = ..., layout: Any = ..., name: Optional[Any] = ...): ...

Prelinearize: Any

def prelinearize_eager_fallback(input: Any, shape: Any, layout: Any, name: Any, ctx: Any): ...
def prelinearize_tuple(inputs: Any, shapes: Any, layouts: Any = ..., name: Optional[Any] = ...): ...

PrelinearizeTuple: Any

def prelinearize_tuple_eager_fallback(inputs: Any, shapes: Any, layouts: Any, name: Any, ctx: Any): ...
def recv_tpu_embedding_activations(num_outputs: Any, config: Any, name: Optional[Any] = ...): ...

RecvTPUEmbeddingActivations: Any

def recv_tpu_embedding_activations_eager_fallback(num_outputs: Any, config: Any, name: Any, ctx: Any): ...

_RetrieveTPUEmbeddingADAMParametersOutput = namedtuple('RetrieveTPUEmbeddingADAMParameters', ['parameters', 'momenta', 'velocities'])

def retrieve_tpu_embedding_adam_parameters(num_shards: Any, shard_id: Any, table_id: int = ..., table_name: str = ..., config: str = ..., name: Optional[Any] = ...): ...

RetrieveTPUEmbeddingADAMParameters: Any

def retrieve_tpu_embedding_adam_parameters_eager_fallback(num_shards: Any, shard_id: Any, table_id: Any, table_name: Any, config: Any, name: Any, ctx: Any): ...

_RetrieveTPUEmbeddingADAMParametersGradAccumDebugOutput = namedtuple('RetrieveTPUEmbeddingADAMParametersGradAccumDebug', ['parameters', 'momenta', 'velocities', 'gradient_accumulators'])

def retrieve_tpu_embedding_adam_parameters_grad_accum_debug(num_shards: Any, shard_id: Any, table_id: int = ..., table_name: str = ..., config: str = ..., name: Optional[Any] = ...): ...

RetrieveTPUEmbeddingADAMParametersGradAccumDebug: Any

def retrieve_tpu_embedding_adam_parameters_grad_accum_debug_eager_fallback(num_shards: Any, shard_id: Any, table_id: Any, table_name: Any, config: Any, name: Any, ctx: Any): ...

_RetrieveTPUEmbeddingAdadeltaParametersOutput = namedtuple('RetrieveTPUEmbeddingAdadeltaParameters', ['parameters', 'accumulators', 'updates'])

def retrieve_tpu_embedding_adadelta_parameters(num_shards: Any, shard_id: Any, table_id: int = ..., table_name: str = ..., config: str = ..., name: Optional[Any] = ...): ...

RetrieveTPUEmbeddingAdadeltaParameters: Any

def retrieve_tpu_embedding_adadelta_parameters_eager_fallback(num_shards: Any, shard_id: Any, table_id: Any, table_name: Any, config: Any, name: Any, ctx: Any): ...

_RetrieveTPUEmbeddingAdadeltaParametersGradAccumDebugOutput = namedtuple('RetrieveTPUEmbeddingAdadeltaParametersGradAccumDebug', ['parameters', 'accumulators', 'updates', 'gradient_accumulators'])

def retrieve_tpu_embedding_adadelta_parameters_grad_accum_debug(num_shards: Any, shard_id: Any, table_id: int = ..., table_name: str = ..., config: str = ..., name: Optional[Any] = ...): ...

RetrieveTPUEmbeddingAdadeltaParametersGradAccumDebug: Any

def retrieve_tpu_embedding_adadelta_parameters_grad_accum_debug_eager_fallback(num_shards: Any, shard_id: Any, table_id: Any, table_name: Any, config: Any, name: Any, ctx: Any): ...

_RetrieveTPUEmbeddingAdagradParametersOutput = namedtuple('RetrieveTPUEmbeddingAdagradParameters', ['parameters', 'accumulators'])

def retrieve_tpu_embedding_adagrad_parameters(num_shards: Any, shard_id: Any, table_id: int = ..., table_name: str = ..., config: str = ..., name: Optional[Any] = ...): ...

RetrieveTPUEmbeddingAdagradParameters: Any

def retrieve_tpu_embedding_adagrad_parameters_eager_fallback(num_shards: Any, shard_id: Any, table_id: Any, table_name: Any, config: Any, name: Any, ctx: Any): ...

_RetrieveTPUEmbeddingAdagradParametersGradAccumDebugOutput = namedtuple('RetrieveTPUEmbeddingAdagradParametersGradAccumDebug', ['parameters', 'accumulators', 'gradient_accumulators'])

def retrieve_tpu_embedding_adagrad_parameters_grad_accum_debug(num_shards: Any, shard_id: Any, table_id: int = ..., table_name: str = ..., config: str = ..., name: Optional[Any] = ...): ...

RetrieveTPUEmbeddingAdagradParametersGradAccumDebug: Any

def retrieve_tpu_embedding_adagrad_parameters_grad_accum_debug_eager_fallback(num_shards: Any, shard_id: Any, table_id: Any, table_name: Any, config: Any, name: Any, ctx: Any): ...

_RetrieveTPUEmbeddingCenteredRMSPropParametersOutput = namedtuple('RetrieveTPUEmbeddingCenteredRMSPropParameters', ['parameters', 'ms', 'mom', 'mg'])

def retrieve_tpu_embedding_centered_rms_prop_parameters(num_shards: Any, shard_id: Any, table_id: int = ..., table_name: str = ..., config: str = ..., name: Optional[Any] = ...): ...

RetrieveTPUEmbeddingCenteredRMSPropParameters: Any

def retrieve_tpu_embedding_centered_rms_prop_parameters_eager_fallback(num_shards: Any, shard_id: Any, table_id: Any, table_name: Any, config: Any, name: Any, ctx: Any): ...

_RetrieveTPUEmbeddingFTRLParametersOutput = namedtuple('RetrieveTPUEmbeddingFTRLParameters', ['parameters', 'accumulators', 'linears'])

def retrieve_tpu_embedding_ftrl_parameters(num_shards: Any, shard_id: Any, table_id: int = ..., table_name: str = ..., config: str = ..., name: Optional[Any] = ...): ...

RetrieveTPUEmbeddingFTRLParameters: Any

def retrieve_tpu_embedding_ftrl_parameters_eager_fallback(num_shards: Any, shard_id: Any, table_id: Any, table_name: Any, config: Any, name: Any, ctx: Any): ...

_RetrieveTPUEmbeddingFTRLParametersGradAccumDebugOutput = namedtuple('RetrieveTPUEmbeddingFTRLParametersGradAccumDebug', ['parameters', 'accumulators', 'linears', 'gradient_accumulators'])

def retrieve_tpu_embedding_ftrl_parameters_grad_accum_debug(num_shards: Any, shard_id: Any, table_id: int = ..., table_name: str = ..., config: str = ..., name: Optional[Any] = ...): ...

RetrieveTPUEmbeddingFTRLParametersGradAccumDebug: Any

def retrieve_tpu_embedding_ftrl_parameters_grad_accum_debug_eager_fallback(num_shards: Any, shard_id: Any, table_id: Any, table_name: Any, config: Any, name: Any, ctx: Any): ...

_RetrieveTPUEmbeddingMDLAdagradLightParametersOutput = namedtuple('RetrieveTPUEmbeddingMDLAdagradLightParameters', ['parameters', 'accumulators', 'weights', 'benefits'])

def retrieve_tpu_embedding_mdl_adagrad_light_parameters(num_shards: Any, shard_id: Any, table_id: int = ..., table_name: str = ..., config: str = ..., name: Optional[Any] = ...): ...

RetrieveTPUEmbeddingMDLAdagradLightParameters: Any

def retrieve_tpu_embedding_mdl_adagrad_light_parameters_eager_fallback(num_shards: Any, shard_id: Any, table_id: Any, table_name: Any, config: Any, name: Any, ctx: Any): ...

_RetrieveTPUEmbeddingMomentumParametersOutput = namedtuple('RetrieveTPUEmbeddingMomentumParameters', ['parameters', 'momenta'])

def retrieve_tpu_embedding_momentum_parameters(num_shards: Any, shard_id: Any, table_id: int = ..., table_name: str = ..., config: str = ..., name: Optional[Any] = ...): ...

RetrieveTPUEmbeddingMomentumParameters: Any

def retrieve_tpu_embedding_momentum_parameters_eager_fallback(num_shards: Any, shard_id: Any, table_id: Any, table_name: Any, config: Any, name: Any, ctx: Any): ...

_RetrieveTPUEmbeddingMomentumParametersGradAccumDebugOutput = namedtuple('RetrieveTPUEmbeddingMomentumParametersGradAccumDebug', ['parameters', 'momenta', 'gradient_accumulators'])

def retrieve_tpu_embedding_momentum_parameters_grad_accum_debug(num_shards: Any, shard_id: Any, table_id: int = ..., table_name: str = ..., config: str = ..., name: Optional[Any] = ...): ...

RetrieveTPUEmbeddingMomentumParametersGradAccumDebug: Any

def retrieve_tpu_embedding_momentum_parameters_grad_accum_debug_eager_fallback(num_shards: Any, shard_id: Any, table_id: Any, table_name: Any, config: Any, name: Any, ctx: Any): ...

_RetrieveTPUEmbeddingProximalAdagradParametersOutput = namedtuple('RetrieveTPUEmbeddingProximalAdagradParameters', ['parameters', 'accumulators'])

def retrieve_tpu_embedding_proximal_adagrad_parameters(num_shards: Any, shard_id: Any, table_id: int = ..., table_name: str = ..., config: str = ..., name: Optional[Any] = ...): ...

RetrieveTPUEmbeddingProximalAdagradParameters: Any

def retrieve_tpu_embedding_proximal_adagrad_parameters_eager_fallback(num_shards: Any, shard_id: Any, table_id: Any, table_name: Any, config: Any, name: Any, ctx: Any): ...

_RetrieveTPUEmbeddingProximalAdagradParametersGradAccumDebugOutput = namedtuple('RetrieveTPUEmbeddingProximalAdagradParametersGradAccumDebug', ['parameters', 'accumulators', 'gradient_accumulators'])

def retrieve_tpu_embedding_proximal_adagrad_parameters_grad_accum_debug(num_shards: Any, shard_id: Any, table_id: int = ..., table_name: str = ..., config: str = ..., name: Optional[Any] = ...): ...

RetrieveTPUEmbeddingProximalAdagradParametersGradAccumDebug: Any

def retrieve_tpu_embedding_proximal_adagrad_parameters_grad_accum_debug_eager_fallback(num_shards: Any, shard_id: Any, table_id: Any, table_name: Any, config: Any, name: Any, ctx: Any): ...

_RetrieveTPUEmbeddingRMSPropParametersOutput = namedtuple('RetrieveTPUEmbeddingRMSPropParameters', ['parameters', 'ms', 'mom'])

def retrieve_tpu_embedding_rms_prop_parameters(num_shards: Any, shard_id: Any, table_id: int = ..., table_name: str = ..., config: str = ..., name: Optional[Any] = ...): ...

RetrieveTPUEmbeddingRMSPropParameters: Any

def retrieve_tpu_embedding_rms_prop_parameters_eager_fallback(num_shards: Any, shard_id: Any, table_id: Any, table_name: Any, config: Any, name: Any, ctx: Any): ...

_RetrieveTPUEmbeddingRMSPropParametersGradAccumDebugOutput = namedtuple('RetrieveTPUEmbeddingRMSPropParametersGradAccumDebug', ['parameters', 'ms', 'mom', 'gradient_accumulators'])

def retrieve_tpu_embedding_rms_prop_parameters_grad_accum_debug(num_shards: Any, shard_id: Any, table_id: int = ..., table_name: str = ..., config: str = ..., name: Optional[Any] = ...): ...

RetrieveTPUEmbeddingRMSPropParametersGradAccumDebug: Any

def retrieve_tpu_embedding_rms_prop_parameters_grad_accum_debug_eager_fallback(num_shards: Any, shard_id: Any, table_id: Any, table_name: Any, config: Any, name: Any, ctx: Any): ...
def retrieve_tpu_embedding_stochastic_gradient_descent_parameters(num_shards: Any, shard_id: Any, table_id: int = ..., table_name: str = ..., config: str = ..., name: Optional[Any] = ...): ...

RetrieveTPUEmbeddingStochasticGradientDescentParameters: Any

def retrieve_tpu_embedding_stochastic_gradient_descent_parameters_eager_fallback(num_shards: Any, shard_id: Any, table_id: Any, table_name: Any, config: Any, name: Any, ctx: Any): ...
def send_tpu_embedding_gradients(inputs: Any, learning_rates: Any, config: Any, name: Optional[Any] = ...): ...

SendTPUEmbeddingGradients: Any

def send_tpu_embedding_gradients_eager_fallback(inputs: Any, learning_rates: Any, config: Any, name: Any, ctx: Any): ...
def shutdown_distributed_tpu(name: Optional[Any] = ...): ...

ShutdownDistributedTPU: Any

def shutdown_distributed_tpu_eager_fallback(name: Any, ctx: Any): ...
def tpu_compilation_result(name: Optional[Any] = ...): ...

TPUCompilationResult: Any

def tpu_compilation_result_eager_fallback(name: Any, ctx: Any): ...
def tpu_embedding_activations(embedding_variable: Any, sliced_activations: Any, table_id: Any, lookup_id: Any, name: Optional[Any] = ...): ...

TPUEmbeddingActivations: Any

def tpu_embedding_activations_eager_fallback(embedding_variable: Any, sliced_activations: Any, table_id: Any, lookup_id: Any, name: Any, ctx: Any): ...
def tpu_ordinal_selector(name: Optional[Any] = ...): ...

TPUOrdinalSelector: Any

def tpu_ordinal_selector_eager_fallback(name: Any, ctx: Any): ...
def tpu_partitioned_call(args: Any, device_ordinal: Any, Tout: Any, f: Any, autotuner_thresh: int = ..., name: Optional[Any] = ...): ...

TPUPartitionedCall: Any

def tpu_partitioned_call_eager_fallback(args: Any, device_ordinal: Any, Tout: Any, f: Any, autotuner_thresh: Any, name: Any, ctx: Any): ...
def tpu_replicate_metadata(num_replicas: Any, num_cores_per_replica: int = ..., topology: str = ..., use_tpu: bool = ..., device_assignment: Any = ..., computation_shape: Any = ..., host_compute_core: Any = ..., padding_map: Any = ..., step_marker_location: str = ..., allow_soft_placement: bool = ..., name: Optional[Any] = ...): ...

TPUReplicateMetadata: Any

def tpu_replicate_metadata_eager_fallback(num_replicas: Any, num_cores_per_replica: Any, topology: Any, use_tpu: Any, device_assignment: Any, computation_shape: Any, host_compute_core: Any, padding_map: Any, step_marker_location: Any, allow_soft_placement: Any, name: Any, ctx: Any): ...
def tpu_replicated_input(inputs: Any, is_mirrored_variable: bool = ..., index: int = ..., name: Optional[Any] = ...): ...

TPUReplicatedInput: Any

def tpu_replicated_input_eager_fallback(inputs: Any, is_mirrored_variable: Any, index: Any, name: Any, ctx: Any): ...
def tpu_replicated_output(input: Any, num_replicas: Any, name: Optional[Any] = ...): ...

TPUReplicatedOutput: Any

def tpu_replicated_output_eager_fallback(input: Any, num_replicas: Any, name: Any, ctx: Any): ...
def worker_heartbeat(request: Any, name: Optional[Any] = ...): ...

WorkerHeartbeat: Any

def worker_heartbeat_eager_fallback(request: Any, name: Any, ctx: Any): ...
