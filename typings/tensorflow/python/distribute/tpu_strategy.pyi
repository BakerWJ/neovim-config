from tensorflow.compiler.xla.experimental.xla_sharding import xla_sharding as xla_sharding
from tensorflow.python.distribute import device_util as device_util, distribute_lib as distribute_lib, input_lib as input_lib, numpy_dataset as numpy_dataset, reduce_util as reduce_util, tpu_values as tpu_values, values as values
from tensorflow.python.distribute.cluster_resolver import TPUClusterResolver as TPUClusterResolver
from tensorflow.python.eager import context as context, def_function as def_function, function as function
from tensorflow.python.framework import constant_op as constant_op, device_spec as device_spec, dtypes as dtypes, ops as ops, tensor_shape as tensor_shape, tensor_util as tensor_util
from tensorflow.python.ops import array_ops as array_ops, control_flow_ops as control_flow_ops, math_ops as math_ops, resource_variable_ops as resource_variable_ops
from tensorflow.python.tpu import tpu as tpu, tpu_strategy_util as tpu_strategy_util, training_loop as training_loop
from tensorflow.python.tpu.ops import tpu_ops as tpu_ops
from tensorflow.python.util import nest as nest
from tensorflow.python.util.tf_export import tf_export as tf_export
from typing import Any, Optional

def get_tpu_system_metadata(tpu_cluster_resolver: Any): ...
def maybe_init_scope() -> None: ...
def validate_run_function(fn: Any) -> None: ...

class TPUStrategy(distribute_lib.Strategy):
    def __init__(self, tpu_cluster_resolver: Optional[Any] = ..., device_assignment: Optional[Any] = ...) -> None: ...
    def run(self, fn: Any, args: Any = ..., kwargs: Optional[Any] = ..., options: Optional[Any] = ...): ...

class TPUStrategyV1(distribute_lib.StrategyV1):
    def __init__(self, tpu_cluster_resolver: Optional[Any] = ..., steps_per_run: Optional[Any] = ..., device_assignment: Optional[Any] = ...) -> None: ...
    @property
    def steps_per_run(self): ...
    def run(self, fn: Any, args: Any = ..., kwargs: Optional[Any] = ..., options: Optional[Any] = ...): ...

class TPUExtended(distribute_lib.StrategyExtendedV1):
    steps_per_run: Any = ...
    experimental_enable_get_next_as_optional: bool = ...
    def __init__(self, container_strategy: Any, tpu_cluster_resolver: Optional[Any] = ..., steps_per_run: Optional[Any] = ..., device_assignment: Optional[Any] = ...) -> None: ...
    def experimental_logical_device(self, logical_device_id: Any) -> None: ...
    def read_var(self, var: Any): ...
    def value_container(self, value: Any): ...
    @property
    def num_hosts(self): ...
    @property
    def num_replicas_per_host(self): ...
    @property
    def experimental_between_graph(self): ...
    @property
    def experimental_should_init(self): ...
    @property
    def should_checkpoint(self): ...
    @property
    def should_save_summary(self): ...
    @property
    def worker_devices(self): ...
    @property
    def parameter_devices(self): ...
    def non_slot_devices(self, var_list: Any): ...
    def tpu_run(self, fn: Any, args: Any, kwargs: Any, options: Optional[Any] = ...): ...

class _TPUReplicaContext(distribute_lib.ReplicaContext):
    def __init__(self, strategy: Any, replica_id_in_sync_group: Optional[Any] = ...) -> None: ...
    @property
    def devices(self): ...
    def experimental_logical_device(self, logical_device_id: Any): ...
